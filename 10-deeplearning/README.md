# 10 - Deep Learning: Reducci贸n de Dimensionalidad y Procesamiento de Lenguaje Natural

Este m贸dulo contiene dos ejemplos pr谩cticos de deep learning:

1. **Comparaci贸n de PCA vs Autoencoder** para reducci贸n de dimensionalidad
2. **Introducci贸n a Hugging Face** para procesamiento de lenguaje natural

##  Contenidos

### 1. PCA_vs_Autoencoder.ipynb
**Comparaci贸n de PCA vs Autoencoder para Reducci贸n de Dimensionalidad**

Este notebook compara dos t茅cnicas de reducci贸n de dimensionalidad aplicadas a datos de presi贸n atmosf茅rica (MSL - Mean Sea Level Pressure):

#### Objetivos:
- Entender las diferencias entre PCA (m茅todo lineal) y Autoencoders (m茅todo no lineal)
- Aplicar ambas t茅cnicas a datos clim谩ticos reales
- Comparar la calidad de reconstrucci贸n de ambos m茅todos
- Analizar la correlaci贸n entre las representaciones latentes

#### Contenido:
1. **Carga de datos**: Datos de presi贸n atmosf茅rica ERA5 del Oc茅ano Pac铆fico
2. **Aplicaci贸n de PCA**: 
   - Reducci贸n de dimensionalidad manteniendo el 90% de la varianza
   - Visualizaci贸n de componentes principales (EOFs)
   - Reconstrucci贸n de datos originales
3. **Aplicaci贸n de Autoencoder CNN**:
   - Preparaci贸n de datos para redes neuronales
   - Entrenamiento de autoencoder con espacio latente de dimensi贸n 15
   - Obtenci贸n de representaci贸n latente
   - Reconstrucci贸n de datos
4. **Comparaci贸n de m茅todos**:
   - Visualizaci贸n de errores de reconstrucci贸n espacial
   - An谩lisis de correlaci贸n entre componentes PCA y dimensiones latentes del autoencoder

#### Requisitos:
- `bluemath_tk` (para PCA y Autoencoders)
- `xarray` (para datos clim谩ticos)
- `cartopy` (para visualizaci贸n geogr谩fica)
- `numpy`, `pandas`, `matplotlib`, `seaborn`

#### Conceptos clave:
- **PCA (Principal Component Analysis)**: M茅todo lineal de reducci贸n de dimensionalidad basado en 谩lgebra lineal
- **Autoencoder**: Red neuronal que aprende representaciones comprimidas de los datos
- **Espacio latente**: Representaci贸n de menor dimensi贸n que captura las caracter铆sticas m谩s importantes
- **Reconstrucci贸n**: Recuperar los datos originales desde la representaci贸n comprimida

---

### 2. what_is_hugging_face.ipynb
**Introducci贸n a Hugging Face y Procesamiento de Lenguaje Natural**

Este notebook proporciona una introducci贸n pr谩ctica a Hugging Face, la plataforma l铆der para modelos de IA pre-entrenados, especialmente en procesamiento de lenguaje natural (NLP).

#### Objetivos:
- Comprender qu茅 es Hugging Face y su ecosistema
- Usar modelos pre-entrenados para diferentes tareas de NLP
- Aplicar transformers a problemas reales de an谩lisis de texto
- Integrar modelos de IA con an谩lisis de datos

#### Contenido:

**1. Introducci贸n a Hugging Face**
- 驴Qu茅 es Hugging Face?
- 驴Por qu茅 es importante para an谩lisis de datos?
- Caracter铆sticas principales de la plataforma

**2. Casos de uso pr谩cticos con c贸digo ejecutable:**

   - **3.1. An谩lisis de Sentimientos**
     - Detectar si un texto expresa opini贸n positiva o negativa
     - Ejemplos con m煤ltiples textos
   
   - **3.2. Clasificaci贸n Zero-Shot**
     - Clasificar texto en categor铆as personalizadas sin entrenar
     - til cuando no tienes datos etiquetados
   
   - **3.3. Generaci贸n de Res煤menes**
     - Condensar textos largos manteniendo informaci贸n clave
     - Usando modelos BART
   
   - **3.4. An谩lisis de Sentimientos en Espa帽ol**
     - Modelos espec铆ficos para espa帽ol
     - Mejores resultados con textos en espa帽ol
   
   - **3.5. Question Answering**
     - Responder preguntas bas谩ndose en un contexto
     - til para sistemas de b煤squeda y chatbots

**3. Instalaci贸n y configuraci贸n**
- Verificaci贸n de librer铆as instaladas
- Tu primer pipeline de Hugging Face

**4. Conexi贸n con proyectos existentes**
- Aplicaciones con datos de COVID-19
- Integraci贸n con an谩lisis de datos HAR
- Procesamiento de logs de sensores

#### Requisitos:
```bash
pip install transformers torch datasets accelerate
```